#!/usr/bin/env python3
"""
RSS æ—¥æŠ¥ç”Ÿæˆå™¨
ä» OPML æ–‡ä»¶è§£æ RSS æºï¼ŒæŠ“å–æ˜¨æ—¥å†…å®¹ï¼Œç”Ÿæˆ AI æ€»ç»“æ—¥æŠ¥
"""

import subprocess
import sys

# è‡ªåŠ¨å®‰è£…ç¼ºå¤±ä¾èµ–
def _ensure_deps():
    required = {'feedparser': 'feedparser', 'pytz': 'pytz'}
    missing = []
    for module in required:
        try:
            __import__(module)
        except ImportError:
            missing.append(required[module])

    if not missing:
        return

    print(f"ğŸ“¦ å®‰è£…ä¾èµ–: {', '.join(missing)}")

    def run(*cmd):
        """æ‰§è¡Œå‘½ä»¤ï¼Œä»»ä½•å¼‚å¸¸éƒ½è¿”å› 1ï¼ˆå¤±è´¥ï¼‰"""
        try:
            return subprocess.call(
                list(cmd),
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
        except (FileNotFoundError, PermissionError, OSError):
            return 1

    # 1. python -m pip
    if run(sys.executable, '-m', 'pip', 'install', '--quiet', *missing) == 0:
        print("âœ… ä¾èµ–å®‰è£…å®Œæˆ\n"); return

    # 2. ensurepip å¼•å¯¼åå†è£…
    if run(sys.executable, '-m', 'ensurepip', '--upgrade') == 0:
        if run(sys.executable, '-m', 'pip', 'install', '--quiet', *missing) == 0:
            print("âœ… ä¾èµ–å®‰è£…å®Œæˆ\n"); return

    # 3. apt-get è£… pip åå†è£…
    print("âš™ï¸  å°è¯• apt-get install python3-pip ...")
    if run('apt-get', 'install', '-y', '-q', 'python3-pip') == 0:
        if run(sys.executable, '-m', 'pip', 'install', '--quiet', *missing) == 0:
            print("âœ… ä¾èµ–å®‰è£…å®Œæˆ\n"); return

    # 4. apt-get ç›´æ¥è£…ç³»ç»ŸåŒ…
    apt_map = {'feedparser': 'python3-feedparser', 'pytz': 'python3-tz'}
    apt_pkgs = [apt_map[m] for m in missing if m in apt_map]
    if apt_pkgs:
        print(f"âš™ï¸  å°è¯• apt-get install {' '.join(apt_pkgs)} ...")
        if run('apt-get', 'install', '-y', '-q', *apt_pkgs) == 0:
            print("âœ… ä¾èµ–å®‰è£…å®Œæˆ\n"); return

    print("âŒ æ— æ³•è‡ªåŠ¨å®‰è£…ä¾èµ–ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œï¼š")
    print(f"   apt-get install -y python3-pip && pip3 install {' '.join(missing)}")
    sys.exit(1)

_ensure_deps()

import xml.etree.ElementTree as ET
import feedparser
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import pytz
import json
from pathlib import Path


def parse_opml(opml_path):
    """è§£æ OPML æ–‡ä»¶ï¼Œè¿”å› RSS æºåˆ—è¡¨"""
    tree = ET.parse(opml_path)
    root = tree.getroot()

    feeds = []
    for outline in root.findall('.//outline[@type="rss"]'):
        feeds.append({
            'title': outline.get('title', outline.get('text', 'Unknown')),
            'url': outline.get('xmlUrl')
        })

    return feeds


def fetch_feed(feed_info, timeout=10):
    """æŠ“å–å•ä¸ª RSS æº"""
    try:
        feed = feedparser.parse(feed_info['url'])

        articles = []
        for entry in feed.entries:
            # æå–å‘å¸ƒæ—¶é—´
            pub_date = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=pytz.UTC)
                except:
                    pass
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                try:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=pytz.UTC)
                except:
                    pass

            # æå–æ‘˜è¦
            summary = ''
            if hasattr(entry, 'summary'):
                summary = entry.summary
            elif hasattr(entry, 'description'):
                summary = entry.description

            # æ¸…ç† HTML æ ‡ç­¾ï¼ˆç®€å•å¤„ç†ï¼‰
            import re
            summary = re.sub(r'<[^>]+>', '', summary)
            summary = summary.strip()

            articles.append({
                'title': entry.get('title', 'No Title'),
                'link': entry.get('link', ''),
                'summary': summary,
                'published': pub_date,
                'source': feed_info['title']
            })

        return {'source': feed_info['title'], 'articles': articles, 'error': None}

    except Exception as e:
        return {'source': feed_info['title'], 'articles': [], 'error': str(e)}


def fetch_all_feeds(feeds, max_workers=20):
    """å¹¶å‘æŠ“å–æ‰€æœ‰ RSS æº"""
    results = []
    total = len(feeds)

    print(f"å¼€å§‹æŠ“å– {total} ä¸ª RSS æº...")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_feed, feed): feed for feed in feeds}

        for i, future in enumerate(as_completed(futures), 1):
            result = future.result()
            results.append(result)

            # å®æ—¶åé¦ˆè¿›åº¦
            success = len([r for r in results if not r['error']])
            failed = len([r for r in results if r['error']])
            print(f"è¿›åº¦: {i}/{total} | æˆåŠŸ: {success} | å¤±è´¥: {failed}", end='\r')

    print()  # æ¢è¡Œ
    return results


def filter_yesterday_articles(all_results, days_back=1):
    """è¿‡æ»¤å‡ºæ˜¨æ—¥ï¼ˆæˆ–æœ€è¿‘ N å¤©ï¼‰çš„æ–‡ç« """

    # è®¡ç®—ç›®æ ‡æ—¥æœŸèŒƒå›´ï¼ˆä½¿ç”¨æœ¬åœ°æ—¶åŒºï¼‰
    now = datetime.now(pytz.timezone('Asia/Shanghai'))
    start_date = (now - timedelta(days=days_back)).replace(hour=0, minute=0, second=0, microsecond=0)
    end_date = now

    filtered = []

    for result in all_results:
        if result['error']:
            continue

        for article in result['articles']:
            if article['published']:
                # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒº
                local_time = article['published'].astimezone(pytz.timezone('Asia/Shanghai'))
                if start_date <= local_time <= end_date:
                    article['published_local'] = local_time
                    filtered.append(article)

    # æŒ‰æ—¶é—´å€’åºæ’åº
    filtered.sort(key=lambda x: x['published'], reverse=True)

    return filtered, start_date, end_date


def prepare_articles_json(articles, start_date, end_date):
    """å‡†å¤‡æ–‡ç« æ•°æ®çš„ JSON æ ¼å¼ï¼Œä¾› AI æ€»ç»“"""

    # æŒ‰æ¥æºåˆ†ç»„ç»Ÿè®¡
    source_stats = {}
    for article in articles:
        source = article['source']
        source_stats[source] = source_stats.get(source, 0) + 1

    # æ„å»ºæ–‡ç« åˆ—è¡¨ï¼ˆé™åˆ¶å‰ 150 ç¯‡ï¼‰
    articles_data = []
    for article in articles[:150]:
        pub_time = article['published_local'].strftime('%Y-%m-%d %H:%M')
        articles_data.append({
            'title': article['title'],
            'source': article['source'],
            'time': pub_time,
            'link': article['link'],
            'summary': article['summary'][:300]  # é™åˆ¶æ‘˜è¦é•¿åº¦
        })

    # æ¥æºç»Ÿè®¡ Top 15
    top_sources = sorted(source_stats.items(), key=lambda x: x[1], reverse=True)[:15]

    return {
        'meta': {
            'start_date': start_date.strftime('%Y-%m-%d'),
            'end_date': end_date.strftime('%Y-%m-%d %H:%M'),
            'total_articles': len(articles),
            'total_sources': len(source_stats),
            'top_sources': [{'name': s[0], 'count': s[1]} for s in top_sources]
        },
        'articles': articles_data
    }


def generate_ai_prompt(data):
    """ç”Ÿæˆ AI æ€»ç»“çš„ prompt"""

    meta = data['meta']
    articles = data['articles']

    # æ„å»ºæ–‡ç« åˆ—è¡¨æ–‡æœ¬
    articles_text = []
    for i, article in enumerate(articles, 1):
        articles_text.append(
            f"{i}. **{article['title']}**\n"
            f"   æ¥æº: {article['source']} | æ—¶é—´: {article['time']}\n"
            f"   é“¾æ¥: {article['link']}\n"
            f"   æ‘˜è¦: {article['summary']}\n"
        )

    # æ¥æºç»Ÿè®¡
    sources_text = '\n'.join([f"- {s['name']}: {s['count']} ç¯‡" for s in meta['top_sources']])

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI èµ„è®¯ç¼–è¾‘ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ RSS è®¢é˜…æºçš„æ˜¨æ—¥æ›´æ–°å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–æ—¥æŠ¥ã€‚

**æ—¶é—´èŒƒå›´**: {meta['start_date']} è‡³ {meta['end_date']}
**æ–‡ç« æ€»æ•°**: {meta['total_articles']} ç¯‡
**æ¥æºæ•°é‡**: {meta['total_sources']} ä¸ª

**æ¥æºåˆ†å¸ƒ Top 15**:
{sources_text}

---

**æ–‡ç« åˆ—è¡¨**:

{chr(10).join(articles_text)}

---

**ä»»åŠ¡è¦æ±‚**:

1. **è¯†åˆ«æ ¸å¿ƒä¸»é¢˜**ï¼ˆ4-6 ä¸ªï¼‰ï¼Œå¦‚ï¼šå¤§æ¨¡å‹å‘å¸ƒã€AI åº”ç”¨ã€å¼€æºé¡¹ç›®ã€è¡Œä¸šåŠ¨æ€ã€æŠ€æœ¯çªç ´ã€æ”¿ç­–ç›‘ç®¡ç­‰
2. **æ¯ä¸ªä¸»é¢˜ä¸‹é€‰å‡º 3-5 ç¯‡æœ€é‡è¦çš„æ–‡ç« **ï¼Œæä¾›ï¼š
   - æ ‡é¢˜ï¼ˆä¿ç•™åŸæ ‡é¢˜ï¼‰
   - åŸæ–‡é“¾æ¥ï¼ˆå¿…é¡»ä½¿ç”¨ä¸Šé¢æä¾›çš„çœŸå®é“¾æ¥ï¼‰
   - ä¸€å¥è¯æ€»ç»“ï¼ˆ20-40 å­—ï¼Œæç‚¼æ ¸å¿ƒä¿¡æ¯ï¼‰
3. **ç”Ÿæˆã€Œä»Šæ—¥è¦ç‚¹ã€**ï¼ˆ3-5 æ¡ï¼Œæ¯æ¡ 1 å¥è¯æ¦‚æ‹¬æœ€é‡è¦çš„ä¿¡æ¯ï¼‰
4. **è¾“å‡ºæ ¼å¼**ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ¨¡æ¿

---

**è¾“å‡ºæ¨¡æ¿**:

```markdown
# ğŸ“° RSS æ—¥æŠ¥ Â· {meta['start_date']}

**æ—¶é—´èŒƒå›´**: {meta['start_date']} 00:00 - {meta['end_date']}
**æ–‡ç« æ€»æ•°**: {meta['total_articles']} ç¯‡
**æ¥æºæ•°é‡**: {meta['total_sources']} ä¸ª

---

## ğŸ”¥ [ä¸»é¢˜ 1 åç§°ï¼Œå¦‚ï¼šå¤§æ¨¡å‹å‘å¸ƒ]

### 1. [æ–‡ç« æ ‡é¢˜]
ğŸ”— [åŸæ–‡é“¾æ¥]
ğŸ“ [ä¸€å¥è¯æ€»ç»“ï¼Œ20-40å­—]

### 2. [æ–‡ç« æ ‡é¢˜]
ğŸ”— [åŸæ–‡é“¾æ¥]
ğŸ“ [ä¸€å¥è¯æ€»ç»“]

---

## ğŸ”¬ [ä¸»é¢˜ 2 åç§°ï¼Œå¦‚ï¼šæŠ€æœ¯çªç ´]

### 1. [æ–‡ç« æ ‡é¢˜]
ğŸ”— [åŸæ–‡é“¾æ¥]
ğŸ“ [ä¸€å¥è¯æ€»ç»“]

---

## ğŸ’¡ [ä¸»é¢˜ 3 åç§°]

...

---

## ğŸ¯ ä»Šæ—¥è¦ç‚¹

1. [æœ€é‡è¦çš„ä¿¡æ¯ï¼Œ1 å¥è¯ï¼Œ30-50å­—]
2. [ç¬¬äºŒé‡è¦ï¼Œ1 å¥è¯]
3. [ç¬¬ä¸‰é‡è¦ï¼Œ1 å¥è¯]
4. [å€¼å¾—å…³æ³¨çš„è¶‹åŠ¿ï¼Œ1 å¥è¯]

---

## ğŸ“Š æ¥æºç»Ÿè®¡ Top 15

| æ¥æº | æ–‡ç« æ•° |
|------|--------|
{chr(10).join([f"| {s['name']} | {s['count']} |" for s in meta['top_sources']])}

---
*ç”Ÿæˆæ—¶é—´: {datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')}*
```

**æ³¨æ„äº‹é¡¹**:
- ä¸è¦è‡†é€ å†…å®¹ï¼Œæ‰€æœ‰ä¿¡æ¯å¿…é¡»æ¥è‡ªä¸Šè¿°æ–‡ç« åˆ—è¡¨
- é“¾æ¥å¿…é¡»ä½¿ç”¨åŸæ–‡é“¾æ¥ï¼Œä¸å¾—ä¿®æ”¹æˆ–ä¼ªé€ 
- ä¸»é¢˜åˆ†ç±»è¦åˆç†ï¼Œé¿å…è¿‡äºåˆ†æ•£æˆ–é‡å 
- å¦‚æœæŸä¸ªä¸»é¢˜æ–‡ç« ä¸è¶³ 3 ç¯‡ï¼Œå¯ä»¥åªåˆ—å‡ºå®é™…æ•°é‡
- ä¸€å¥è¯æ€»ç»“è¦ç²¾ç‚¼ï¼Œçªå‡ºæ ¸å¿ƒä»·å€¼ï¼Œä¸è¦åªæ˜¯é‡å¤æ ‡é¢˜
- ä»Šæ—¥è¦ç‚¹è¦æç‚¼æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä¸æ˜¯ç®€å•ç½—åˆ—
"""

    return prompt


def resolve_opml_path():
    """æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾ OPML æ–‡ä»¶è·¯å¾„"""
    import os
    candidates = [
        os.environ.get('OPML_PATH', ''),                          # 1. ç¯å¢ƒå˜é‡
        str(Path(__file__).parent.parent / 'BestBlogs_RSS_ALL.opml'),  # 2. skill ç›®å½•ï¼ˆæœ€å¯é ï¼‰
        '/Users/donghan/Downloads/BestBlogs_RSS_ALL.opml',        # 3. Mac æœ¬åœ°è·¯å¾„
        '/home/node/Downloads/BestBlogs_RSS_ALL.opml',            # 4. Docker å®¹å™¨è·¯å¾„
        '/root/Downloads/BestBlogs_RSS_ALL.opml',                 # 5. root ç”¨æˆ·è·¯å¾„
        str(Path.home() / 'Downloads' / 'BestBlogs_RSS_ALL.opml'),# 6. å½“å‰ç”¨æˆ· Downloads
        str(Path(__file__).parent.parent / 'BestBlogs_RSS_ALL.opml'),  # 6. skill ç›®å½•
    ]
    for path in candidates:
        if path and Path(path).exists():
            return path
    return None


def main():
    """ä¸»å‡½æ•°"""

    days_back = 1

    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼špython3 generate_rss_report.py [å¤©æ•°] [opmlè·¯å¾„]
    if len(sys.argv) > 1:
        try:
            days_back = int(sys.argv[1])
        except:
            print(f"æ— æ•ˆçš„å¤©æ•°å‚æ•°: {sys.argv[1]}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1")

    # è§£æ OPML è·¯å¾„
    opml_path = sys.argv[2] if len(sys.argv) > 2 else resolve_opml_path()
    if not opml_path:
        print("âŒ æ‰¾ä¸åˆ° OPML æ–‡ä»¶ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼æŒ‡å®šè·¯å¾„ï¼š")
        print("   1. ç¯å¢ƒå˜é‡: export OPML_PATH=/path/to/file.opml")
        print("   2. å‘½ä»¤è¡Œå‚æ•°: python3 generate_rss_report.py 1 /path/to/file.opml")
        sys.exit(1)

    print("=" * 60)
    print("ğŸ“° RSS æ—¥æŠ¥ç”Ÿæˆå™¨")
    print("=" * 60)
    print()

    # Phase 1: è§£æ OPML
    print("ğŸ“– Phase 1: è§£æ OPML...")
    feeds = parse_opml(opml_path)
    print(f"âœ… æ‰¾åˆ° {len(feeds)} ä¸ª RSS æº\n")

    # Phase 2: æŠ“å– RSS Feed
    print("ğŸŒ Phase 2: æŠ“å– RSS Feed...")
    results = fetch_all_feeds(feeds, max_workers=30)

    success_count = len([r for r in results if not r['error']])
    failed_count = len([r for r in results if r['error']])
    total_articles = sum([len(r['articles']) for r in results if not r['error']])

    print(f"âœ… æŠ“å–å®Œæˆ: æˆåŠŸ {success_count} | å¤±è´¥ {failed_count} | æ€»æ–‡ç« æ•° {total_articles}\n")

    # Phase 3: è¿‡æ»¤æ˜¨æ—¥å†…å®¹
    print(f"ğŸ” Phase 3: è¿‡æ»¤æœ€è¿‘ {days_back} å¤©çš„å†…å®¹...")
    filtered_articles, start_date, end_date = filter_yesterday_articles(results, days_back)
    print(f"âœ… æ‰¾åˆ° {len(filtered_articles)} ç¯‡æ–‡ç« \n")

    if len(filtered_articles) == 0:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ï¼Œè¯·æ£€æŸ¥æ—¥æœŸèŒƒå›´æˆ– RSS æº")
        return

    # Phase 4: å‡†å¤‡æ•°æ®
    print("ğŸ“Š Phase 4: å‡†å¤‡æ•°æ®...")
    data = prepare_articles_json(filtered_articles, start_date, end_date)

    # ä¿å­˜æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
    output_dir = Path(__file__).parent.parent / 'output'
    output_dir.mkdir(exist_ok=True)

    data_file = output_dir / 'rss_data.json'
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {data_file}\n")

    # Phase 5: ç”Ÿæˆ AI Prompt
    print("ğŸ¤– Phase 5: ç”Ÿæˆ AI Prompt...")
    prompt = generate_ai_prompt(data)

    prompt_file = output_dir / 'ai_prompt.txt'
    with open(prompt_file, 'w', encoding='utf-8') as f:
        f.write(prompt)

    print(f"âœ… Prompt å·²ä¿å­˜åˆ°: {prompt_file}\n")

    print("=" * 60)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print()
    print("ğŸ“ ä¸‹ä¸€æ­¥ï¼šå°† ai_prompt.txt çš„å†…å®¹å‘é€ç»™ AI æ¨¡å‹è¿›è¡Œæ€»ç»“")
    print()

    # è¾“å‡º promptï¼ˆä¾› Claude ç›´æ¥ä½¿ç”¨ï¼‰
    print("=" * 60)
    print("AI PROMPT (å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨)")
    print("=" * 60)
    print()
    print(prompt)


if __name__ == '__main__':
    main()
